#Automated Image Captioning

In this project, the paper called "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" is implemented. The combination of CNN and LSTM is used and the dataset used is MS-COCO dataset. Basically, the image goes to the model as input and the caption will be generated as output. Therefore, for encoding CNN is used and for decoding LSTM is used. 
At the end, beam search is also used to enhance the prediction result.


<img width="462" alt="Capture 2" src="https://user-images.githubusercontent.com/42817026/153704996-2fdb0a9f-1d81-457a-aa97-9da75dc4ffe0.PNG">


<img width="419" alt="Capture" src="https://user-images.githubusercontent.com/42817026/153705018-26d835cc-4c85-491f-94c5-46944756b4c8.PNG">
